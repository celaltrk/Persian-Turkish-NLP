{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPg3BY2iilxCN8kY+Do6rzI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/celaltrk/Persian-Turkish-NLP/blob/main/Turkish_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim==4.3.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cprZgckZp_Xi",
        "outputId": "b8aa2327-ec80-4d0b-f2a7-8d383f5f735e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim==4.3.2\n",
            "  Downloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (1.15.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.2) (1.17.2)\n",
            "Downloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "L2qo-5I-Yutg",
        "outputId": "eac20716-62f6-40ae-e412-9c5873a9580b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'triu' from 'scipy.linalg' (/usr/local/lib/python3.11/dist-packages/scipy/linalg/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-836069462b09>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_blas_funcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlapack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpsi\u001b[0m  \u001b[0;31m# gamma function utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (/usr/local/lib/python3.11/dist-packages/scipy/linalg/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from gensim.models import FastText\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Configuration ---\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/turkish_data\")\n",
        "MODEL_OUTPUT_DIR = Path(\"/content/drive/MyDrive/fasttext_models\")\n",
        "MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "# FastText Model Parameters (adjust as needed)\n",
        "VECTOR_SIZE = 100  # Dimensionality of the word vectors\n",
        "WINDOW_SIZE = 5    # Context window size\n",
        "MIN_WORD_COUNT = 5 # Minimum word frequency to consider\n",
        "SKIP_GRAM = 1      # 1 for Skip-gram, 0 for CBOW. Skip-gram is generally better.\n",
        "EPOCHS = 15        # Number of training iterations over the corpus\n",
        "MIN_N_CHAR_GRAM = 3 # Minimum length of char n-grams\n",
        "MAX_N_CHAR_GRAM = 6 # Maximum length of char n-grams\n",
        "NUM_WORKERS = os.cpu_count() if os.cpu_count() else 4 # Use available CPU cores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_turkish_text(text_lines):\n",
        "    \"\"\"\n",
        "    Preprocesses a list of text lines for Turkish:\n",
        "    1. Converts to lowercase.\n",
        "    2. Removes punctuation and digits.\n",
        "    3. Tokenizes by whitespace.\n",
        "    Returns a list of lists of tokens (sentences).\n",
        "    \"\"\"\n",
        "    processed_sentences = []\n",
        "    for line in tqdm(text_lines, desc=\"Preprocessing text lines\"):\n",
        "        # Convert to lowercase (handles Turkish 'İ'->'i', 'I'->'ı')\n",
        "        line = line.lower()\n",
        "        # Remove punctuation and digits - keep letters and spaces\n",
        "        # This regex keeps Turkish characters (ç, ğ, ı, ö, ş, ü)\n",
        "        line = re.sub(r'[^\\w\\sığüşöç]', '', line) # Remove punctuation except Turkish chars\n",
        "        line = re.sub(r'\\d+', '', line) # Remove digits\n",
        "\n",
        "        tokens = line.split() # Tokenize by whitespace\n",
        "        if tokens: # Add sentence only if it's not empty after preprocessing\n",
        "            processed_sentences.append(tokens)\n",
        "    return processed_sentences"
      ],
      "metadata": {
        "id": "URAmxij3e5t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_decade_model(text_file_path):\n",
        "    \"\"\"\n",
        "    Trains a FastText model for a single decade's text file.\n",
        "    \"\"\"\n",
        "    decade_name = text_file_path.stem  # e.g., \"1930s\" from \"1930s.txt\"\n",
        "    logging.info(f\"--- Processing decade: {decade_name} ---\")\n",
        "\n",
        "    # Read the text file\n",
        "    try:\n",
        "        with open(text_file_path, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Could not read file {text_file_path}: {e}\")\n",
        "        return\n",
        "\n",
        "    if not lines:\n",
        "        logging.warning(f\"File {text_file_path} is empty. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # Preprocess the text\n",
        "    logging.info(f\"Preprocessing text for {decade_name}...\")\n",
        "    sentences = preprocess_turkish_text(lines)\n",
        "\n",
        "    if not sentences:\n",
        "        logging.warning(f\"No processable sentences found in {text_file_path} for {decade_name}. Skipping model training.\")\n",
        "        return\n",
        "\n",
        "    # Train the FastText model\n",
        "    logging.info(f\"Training FastText model for {decade_name}...\")\n",
        "    model = FastText(\n",
        "        sentences=sentences,\n",
        "        vector_size=VECTOR_SIZE,\n",
        "        window=WINDOW_SIZE,\n",
        "        min_count=MIN_WORD_COUNT,\n",
        "        sg=SKIP_GRAM,\n",
        "        epochs=EPOCHS,\n",
        "        min_n=MIN_N_CHAR_GRAM,\n",
        "        max_n=MAX_N_CHAR_GRAM,\n",
        "        workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # Save the trained model\n",
        "    model_save_path = MODEL_OUTPUT_DIR / f\"fasttext_{decade_name}.model\"\n",
        "    try:\n",
        "        model.save(str(model_save_path))\n",
        "        logging.info(f\"Model for {decade_name} saved to {model_save_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Could not save model for {decade_name}: {e}\")\n",
        "\n",
        "    logging.info(f\"--- Finished processing for decade: {decade_name} ---\")"
      ],
      "metadata": {
        "id": "8ivg6ZhMe0ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "# sample_data = {\n",
        "#     \"1930s.txt\": [\n",
        "#         \"Türkiye cumhuriyeti yeni bir döneme girdi.\",\n",
        "#         \"Ekonomi ve sanayi alanında önemli gelişmeler yaşandı.\",\n",
        "#         \"Gazeteler bu haberleri halka duyurdu.\"\n",
        "#     ],\n",
        "#     \"1940s.txt\": [\n",
        "#         \"İkinci dünya savaşı etkileri sürüyordu.\",\n",
        "#         \"Ülkemiz zorlu zamanlardan geçiyordu ama umut vardı.\",\n",
        "#         \"Yeni yasalar meclisten geçti.\"\n",
        "#     ]\n",
        "# }\n",
        "\n",
        "# for filename, content_lines in sample_data.items():\n",
        "#     if not (DATA_DIR / filename).exists(): # Only create if not exists\n",
        "#             with open(DATA_DIR / filename, 'w', encoding='utf-8') as f:\n",
        "#                 for line in content_lines:\n",
        "#                     f.write(line + \"\\n\")\n",
        "#             logging.info(f\"Created dummy file: {DATA_DIR / filename}\")\n",
        "\n",
        "if not DATA_DIR.is_dir():\n",
        "    logging.error(f\"Data directory not found: {DATA_DIR}\")\n",
        "    logging.error(\"Please create a 'data' directory and place your decade .txt files (e.g., 1930s.txt) in it.\")\n",
        "    raise FileNotFoundError(\"Data directory not found\")\n",
        "\n",
        "\n",
        "decade_files = list(DATA_DIR.glob(\"*.txt\"))\n",
        "\n",
        "if not decade_files:\n",
        "    logging.warning(f\"No .txt files found in {DATA_DIR}. Nothing to process.\")\n",
        "    raise FileNotFoundError(\"No .txt files found in data directory\")\n",
        "\n",
        "logging.info(f\"Found {len(decade_files)} decade files to process: {[f.name for f in decade_files]}\")\n",
        "\n",
        "for text_file in sorted(decade_files): # Sort to process in chronological order\n",
        "    train_decade_model(text_file)\n",
        "\n",
        "logging.info(\"All decades processed.\")"
      ],
      "metadata": {
        "id": "CfQUdqrZdZgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example: How to load and use a trained model ---\n",
        "# This is for demonstration after training is complete.\n",
        "# You would run this part separately or after training.\n",
        "\n",
        "# test_model_path = MODEL_OUTPUT_DIR / \"fasttext_1930s.model\"\n",
        "# if test_model_path.exists():\n",
        "#     logging.info(f\"\\n--- Loading and testing model: {test_model_path} ---\")\n",
        "#     loaded_model = FastText.load(str(test_model_path))\n",
        "\n",
        "#     # Get vector for a word\n",
        "#     try:\n",
        "#         word_vector = loaded_model.wv[\"cumhuriyet\"]\n",
        "#         logging.info(f\"Vector for 'cumhuriyet': {word_vector[:5]}...\") # Print first 5 dims\n",
        "#     except KeyError:\n",
        "#         logging.info(\"'cumhuriyet' not in vocabulary of 1930s model (or below min_count).\")\n",
        "\n",
        "#     # Find most similar words\n",
        "#     try:\n",
        "#         similar_words = loaded_model.wv.most_similar(\"ekonomi\", topn=5)\n",
        "#         logging.info(f\"Words similar to 'ekonomi': {similar_words}\")\n",
        "#     except KeyError:\n",
        "#         logging.info(\"'ekonomi' not in vocabulary of 1930s model (or below min_count).\")\n",
        "\n",
        "#     # FastText can also get vectors for OOV words if their n-grams are known\n",
        "#     oov_word = \"yepyenişeyler\" # A made-up word\n",
        "#     oov_vector = loaded_model.wv[oov_word]\n",
        "#     logging.info(f\"Vector for OOV word '{oov_word}': {oov_vector[:5]}...\")\n",
        "# else:\n",
        "#     logging.info(f\"Test model {test_model_path} not found. Run training first.\")\n",
        "# --- End of Example Usage ---"
      ],
      "metadata": {
        "id": "J2Nn7KEQejYy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}