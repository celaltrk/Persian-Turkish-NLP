{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-LpNwXd-ciSr",
    "outputId": "ca1c90ca-01a3-4320-8702-2e35cf78fdd2"
   },
   "outputs": [],
   "source": [
    "!pip install gensim tqdm zemberek-python jpype1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "L2qo-5I-Yutg",
    "outputId": "b05e09fa-3bd5-42a8-98d8-411a134b9182"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from gensim.models import FastText\n",
    "from zemberek import (\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# --- Configuration ---jpype.startJVM(jpype.getDefaultJVMPath(), \"-Djava.class.path=/path/to/zemberek.jar\")\n",
    "\n",
    "DATA_DIR = Path(\"./data\")\n",
    "MODEL_OUTPUT_DIR = Path(\"./embedding_models\")\n",
    "MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# FastText Model Parameters\n",
    "VECTOR_SIZE = 100  # Dimensionality of the word vectors\n",
    "WINDOW_SIZE = 5    # Context window size\n",
    "MIN_WORD_COUNT = 5 # Minimum word frequency to consider\n",
    "SKIP_GRAM = 1      # 1 for Skip-gram, 0 for CBOW. Skip-gram is generally better.\n",
    "EPOCHS = 15        # Number of training iterations over the corpus\n",
    "MIN_N_CHAR_GRAM = 3 # Minimum length of char n-grams\n",
    "MAX_N_CHAR_GRAM = 6 # Maximum length of char n-grams\n",
    "NEGATIVE = 5 # SGNS\n",
    "\n",
    "NUM_WORKERS = os.cpu_count() if os.cpu_count() else 4 # Use available CPU cores\n",
    "NUM_WORKERS = NUM_WORKERS - 3 # prevent excessive load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 21:25:47,542 - root - INFO\n",
      "Msg: Number of workers is 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"Number of workers is {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword list from https://github.com/ahmetax/trstop/blob/master/dosyalar/turkce-stop-words\n",
    "STOPWORDS = [\n",
    "    'acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'bile', 'bir', 'biraz', 'birçoğu',\n",
    "    'birçok', 'birisi', 'birkaç', 'birşey', 'biz', 'bizden', 'bize', 'bizi', 'bizim', 'bu',\n",
    "    'buna', 'bundan', 'bunlar', 'bunları', 'bunların', 'bunu', 'bunun', 'burada', 'böyle',\n",
    "    'böylece', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep',\n",
    "    'hepsi', 'her', 'herkes', 'hiç', 'hiçbir', 'için', 'ile', 'ise', 'içinde', 'kadar', 'ki',\n",
    "    'kim', 'kimse', 'mı', 'mi', 'mu', 'mü', 'nasıl', 'ne', 'neden', 'nerde', 'nereye', 'niye',\n",
    "    'niçin', 'o', 'olan', 'olarak', 'oldu', 'olduğu', 'olmak', 'olmaz', 'olsun', 'on', 'ona',\n",
    "    'ondan', 'onlar', 'onlardan', 'onları', 'onların', 'onu', 'onun', 'orada', 'sanki',\n",
    "    'sadece', 'sen', 'senden', 'sende', 'seni', 'senin', 'siz', 'sizden', 'size', 'sizi',\n",
    "    'sizin', 'şey', 'şu', 'şuna', 'şunda', 'şundan', 'şunları', 'şunlar', 'şunu', 'şunun',\n",
    "    'ta', 'tamam', 'tüm', 've', 'veya', 'ya', 'yani'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 21:25:50,808 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 3.251668691635132\n",
      "\n",
      "2025-05-14 21:26:00,134 - root - INFO\n",
      "Msg: Zemberek tools initialized successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    morphology = TurkishMorphology.create_with_defaults()\n",
    "    normalizer = TurkishSentenceNormalizer(morphology)\n",
    "    spell_checker = TurkishSpellChecker(morphology)\n",
    "    extractor = TurkishSentenceExtractor()\n",
    "    tokenizer = TurkishTokenizer.DEFAULT\n",
    "    logging.info(\"Zemberek tools initialized successfully\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to initialize Zemberek: {e}\")\n",
    "    morphology = normalizer = spell_checker = extractor = tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "URAmxij3e5t_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|█████████████████████████████| 2/2 [00:00<00:00, 117.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 21:35:20,932 - root - INFO\n",
      "Msg: Processed 0 sentences with 0 unique words\n",
      "\n",
      "2025-05-14 21:35:20,933 - root - INFO\n",
      "Msg: Preprocessed sentences saved to embedding_models/preprocessed_test.txt\n",
      "\n",
      "[['bugün', 'dün', 'iyi', 'yapmak', 'büyümek', 'değer'], ['mut', 'sırrı', 'mut', 'geçer']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_turkish_text(text_lines):\n",
    "    processed_sentences = []\n",
    "    vocab_counts = Counter()\n",
    "\n",
    "    for line_num, line in enumerate(tqdm(text_lines, desc=\"Preprocessing\")):\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        \n",
    "        sentences = extractor.from_paragraph(line.strip())\n",
    "        unwanted_token_types = ['Punctuation', 'Emoticon', 'UnknownWord', 'Number', 'SpaceTab', 'NewLine', 'RomanNumeral', 'PercentNumeral', 'Time', 'Date', 'URL', 'Email', 'HashTag', 'Mention', 'MetaTag', 'Emoji', 'Emoticon', 'UnknownWord', 'Unknown']\n",
    "        for sentence in sentences:\n",
    "            # Tokenization\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            token_list = [token.content for token in tokens if token.type_.name not in unwanted_token_types and token.content.lower() not in STOPWORDS]\n",
    "            if not token_list:\n",
    "                continue\n",
    "            \n",
    "            normalized_text = normalizer.normalize(' '.join(token_list))\n",
    "            normalized_tokens = tokenizer.tokenize(normalized_text)\n",
    "            normalized_tokens = [token.content.lower() for token in normalized_tokens if token.type_.name not in unwanted_token_types and token.content.lower() not in STOPWORDS]\n",
    "            \n",
    "            # Spell correction\n",
    "            corrected_tokens = []\n",
    "            for token in normalized_tokens:\n",
    "                results = morphology.analyze(token)\n",
    "                if not results or len(str(results)) < 5:\n",
    "                    suggestions = spell_checker.suggest_for_word(token)\n",
    "                    token = suggestions[0] if suggestions else token\n",
    "                corrected_tokens.append(token)\n",
    "\n",
    "            # Lemmatization\n",
    "            lemmatized_tokens = []\n",
    "            for token in corrected_tokens:\n",
    "                results = morphology.analyze(token)\n",
    "                largest_lemma = ''\n",
    "                largest_lemma_length = 0\n",
    "                for result in results:\n",
    "                    analysis_str = str(result)\n",
    "                    if analysis_str.startswith('['):\n",
    "                        lemma_section = analysis_str.split(']')[0]\n",
    "                        lemma = lemma_section.split(':')[0].lstrip('[')\n",
    "                    else:\n",
    "                        lemma = analysis_str.split(':')[0]\n",
    "                    if (len(lemma) > largest_lemma_length):\n",
    "                        largest_lemma = lemma\n",
    "                if largest_lemma:\n",
    "                    lemmatized_tokens.append(largest_lemma.lower())\n",
    "\n",
    "            final_tokens = [token for token in lemmatized_tokens if token not in STOPWORDS]\n",
    "\n",
    "            # Update vocabulary counts\n",
    "            if final_tokens:\n",
    "                vocab_counts.update(final_tokens)\n",
    "                processed_sentences.append(final_tokens)\n",
    "\n",
    "    # Vocabulary pruning\n",
    "    shared_vocab = {word for word, count in vocab_counts.items() if count >= MIN_WORD_COUNT}\n",
    "    final_sentences = []\n",
    "    for sentence in processed_sentences:\n",
    "        final_sentence = [word if word in shared_vocab else '<UNK>' for word in sentence]\n",
    "        if final_sentence and any(word != '<UNK>' for word in final_sentence):\n",
    "            final_sentences.append(final_sentence)\n",
    "\n",
    "    logging.info(f\"Processed {len(final_sentences)} sentences with {len(shared_vocab)} unique words\")\n",
    "    decade_name = 'test'\n",
    "    preprocessed_file_path = MODEL_OUTPUT_DIR / f\"preprocessed_{decade_name}.txt\"\n",
    "    try:\n",
    "        with open(preprocessed_file_path, 'w', encoding='utf-8') as f:\n",
    "            for sentence in processed_sentences:\n",
    "                f.write(' '.join(sentence) + '\\n')\n",
    "        logging.info(f\"Preprocessed sentences saved to {preprocessed_file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save preprocessed sentences for {decade_name}: {e}\")\n",
    "        \n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all_decade_files(decade_files):\n",
    "    for file_path in decade_files:\n",
    "        decade_name = file_path.stem\n",
    "        logging.info(f\"--- Preprocessing for {decade_name} ---\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Could not read {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if not lines:\n",
    "            logging.warning(f\"{file_path} is empty. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        processed_sentences = preprocess_turkish_text(lines)\n",
    "\n",
    "        if not processed_sentences:\n",
    "            logging.warning(f\"No valid sentences found in {file_path}. Skipping save.\")\n",
    "            continue\n",
    "\n",
    "        save_path = MODEL_OUTPUT_DIR / f\"preprocessed_{decade_name}.txt\"\n",
    "        try:\n",
    "            with open(save_path, 'w', encoding='utf-8') as out_f:\n",
    "                for sentence in processed_sentences:\n",
    "                    out_f.write(' '.join(sentence) + '\\n')\n",
    "            logging.info(f\"Saved preprocessed file: {save_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Could not save preprocessed file {save_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8ivg6ZhMe0ZY"
   },
   "outputs": [],
   "source": [
    "def train_decade_model(text_file_path):\n",
    "    decade_name = text_file_path.stem  # 1930s, 1940s, etc\n",
    "    logging.info(f\"--- Processing decade: {decade_name} ---\")\n",
    "\n",
    "    preprocessed_file_path = MODEL_OUTPUT_DIR / f\"preprocessed_{decade_name}.txt\"\n",
    "    if os.path.exists(preprocessed_file_path):\n",
    "        logging.info(f\"Loading preprocessed sentences from {preprocessed_file_path}...\")\n",
    "        try:\n",
    "            with open(preprocessed_file_path, 'r', encoding='utf-8') as f:\n",
    "                sentences = [line.strip().split() for line in f.readlines()]\n",
    "            logging.info(f\"Loaded {len(sentences)} sentences from preprocessed file.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Could not read preprocessed file {preprocessed_file_path}: {e}\")\n",
    "            return\n",
    "    else:\n",
    "        try:\n",
    "            with open(text_file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Could not read file {text_file_path}: {e}\")\n",
    "            return\n",
    "\n",
    "        if not lines:\n",
    "            logging.warning(f\"File {text_file_path} is empty. Skipping.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Preprocessing text for {decade_name}...\")\n",
    "        sentences = preprocess_turkish_text(lines)\n",
    "        if not sentences:\n",
    "            logging.warning(f\"No processable sentences found in {text_file_path} for {decade_name}. Skipping model training.\")\n",
    "            return\n",
    "\n",
    "    # Train the FastText model\n",
    "    logging.info(f\"Training FastText model for {decade_name}...\")\n",
    "    model = FastText(\n",
    "        sentences=sentences,\n",
    "        vector_size=VECTOR_SIZE,\n",
    "        window=WINDOW_SIZE,\n",
    "        min_count=MIN_WORD_COUNT,\n",
    "        sg=SKIP_GRAM,\n",
    "        negative=NEGATIVE,\n",
    "        epochs=EPOCHS,\n",
    "        min_n=MIN_N_CHAR_GRAM,\n",
    "        max_n=MAX_N_CHAR_GRAM,\n",
    "        workers=NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    # Save the trained model\n",
    "    model_save_path = MODEL_OUTPUT_DIR / f\"fasttext_{decade_name}.model\"\n",
    "    try:\n",
    "        model.save(str(model_save_path))\n",
    "        logging.info(f\"Model for {decade_name} saved to {model_save_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save model for {decade_name}: {e}\")\n",
    "\n",
    "    logging.info(f\"--- Finished processing for decade: {decade_name} ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "CfQUdqrZdZgM",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 21:42:49,620 - root - INFO\n",
      "Msg: Found 7 decade files to process: ['1940s.txt', '1950s.txt', '1930s.txt', '1980s.txt', '1970s.txt', '1960s.txt', '1990s.txt']\n",
      "\n",
      "2025-05-14 21:42:49,620 - root - INFO\n",
      "Msg: --- Preprocessing for 1940s ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:   0%|                 | 10504/4283897 [01:05<7:21:24, 161.35it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo .txt files found in data directory\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(decade_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m decade files to process: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[f.name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mf\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mdecade_files]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mpreprocess_all_decade_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecade_files\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mpreprocess_all_decade_files\u001b[39m\u001b[34m(decade_files)\u001b[39m\n\u001b[32m     16\u001b[39m     logging.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is empty. Skipping.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m processed_sentences = \u001b[43mpreprocess_turkish_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m processed_sentences:\n\u001b[32m     22\u001b[39m     logging.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo valid sentences found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Skipping save.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mpreprocess_turkish_text\u001b[39m\u001b[34m(text_lines)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token_list:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m normalized_text = \u001b[43mnormalizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m normalized_tokens = tokenizer.tokenize(normalized_text)\n\u001b[32m     20\u001b[39m normalized_tokens = [token.content.lower() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m normalized_tokens \u001b[38;5;28;01mif\u001b[39;00m token.type_.name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m unwanted_token_types \u001b[38;5;129;01mand\u001b[39;00m token.content.lower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m STOPWORDS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP_Project/Persian-Turkish-NLP/.venv/lib/python3.12/site-packages/zemberek/normalization/turkish_sentence_normalizer.py:179\u001b[39m, in \u001b[36mTurkishSentenceNormalizer.normalize\u001b[39m\u001b[34m(self, sentence)\u001b[39m\n\u001b[32m    176\u001b[39m                 candidates.append(result.surface)\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(analyses.analysis_results) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(current) > \u001b[32m3\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     spell_candidates = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspell_checker\u001b[49m\u001b[43m.\u001b[49m\u001b[43msuggest_for_word_for_normalization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(spell_candidates) > \u001b[32m3\u001b[39m:\n\u001b[32m    183\u001b[39m         spell_candidates = spell_candidates[:\u001b[32m3\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP_Project/Persian-Turkish-NLP/.venv/lib/python3.12/site-packages/zemberek/normalization/turkish_spell_checker.py:48\u001b[39m, in \u001b[36mTurkishSpellChecker.suggest_for_word_for_normalization\u001b[39m\u001b[34m(self, word, left_context, right_context, lm)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msuggest_for_word_for_normalization\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, left_context: \u001b[38;5;28mstr\u001b[39m, right_context: \u001b[38;5;28mstr\u001b[39m, lm: SmoothLM) -> \\\n\u001b[32m     47\u001b[39m         Tuple[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     unranked: Tuple[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_unranked_suggestions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     50\u001b[39m         logger.warning(\u001b[33m\"\u001b[39m\u001b[33mNo language model provided. Returning unraked results.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP_Project/Persian-Turkish-NLP/.venv/lib/python3.12/site-packages/zemberek/normalization/turkish_spell_checker.py:88\u001b[39m, in \u001b[36mTurkishSpellChecker.get_unranked_suggestions\u001b[39m\u001b[34m(self, word)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_unranked_suggestions\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m) -> Tuple[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m     87\u001b[39m     normalized = TurkishAlphabet.INSTANCE.normalize(re.sub(\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[33m'\u001b[39m\u001b[33m’]\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, word))\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     strings: Tuple[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_suggestions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchar_matcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     case_type = \u001b[38;5;28mself\u001b[39m.formatter.guess_case(word)\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m case_type == WordAnalysisSurfaceFormatter.CaseType.MIXED_CASE \u001b[38;5;129;01mor\u001b[39;00m case_type == \\\n\u001b[32m     91\u001b[39m             WordAnalysisSurfaceFormatter.CaseType.LOWER_CASE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP_Project/Persian-Turkish-NLP/.venv/lib/python3.12/site-packages/zemberek/normalization/character_graph_decoder.py:26\u001b[39m, in \u001b[36mCharacterGraphDecoder.get_suggestions\u001b[39m\u001b[34m(self, input_, matcher)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_suggestions\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_: \u001b[38;5;28mstr\u001b[39m, matcher: \u001b[33m'\u001b[39m\u001b[33mCharacterGraphDecoder.CharMatcher\u001b[39m\u001b[33m'\u001b[39m) -> Tuple[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mCharacterGraphDecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m.keys())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP_Project/Persian-Turkish-NLP/.venv/lib/python3.12/site-packages/zemberek/normalization/character_graph_decoder.py:39\u001b[39m, in \u001b[36mCharacterGraphDecoder.Decoder.decode\u001b[39m\u001b[34m(self, inp)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp: \u001b[38;5;28mstr\u001b[39m) -> Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m     36\u001b[39m     hyp = CharacterGraphDecoder.Hypothesis(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m.outer.graph.root, penalty=\u001b[32m0.0\u001b[39m,\n\u001b[32m     37\u001b[39m                                            operation=CharacterGraphDecoder.Operation.N_A, word=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     38\u001b[39m                                            ending=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     next_: Set[\u001b[33m'\u001b[39m\u001b[33mCharacterGraphDecoder.Hypothesis\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     42\u001b[39m         new_hyps: Set[\u001b[33m'\u001b[39m\u001b[33mCharacterGraphDecoder.Hypothesis\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP_Project/Persian-Turkish-NLP/.venv/lib/python3.12/site-packages/zemberek/normalization/character_graph_decoder.py:106\u001b[39m, in \u001b[36mCharacterGraphDecoder.Decoder.expand\u001b[39m\u001b[34m(self, hypothesis, inp)\u001b[39m\n\u001b[32m    103\u001b[39m     penalty = \u001b[32m1.0\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m penalty > \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hypothesis.penalty + penalty <= \u001b[38;5;28mself\u001b[39m.outer.max_penalty:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     h = \u001b[43mhypothesis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_new_move_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43mCharacterGraphDecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOperation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSUBSTITUTION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     h.set_word(child)\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m next_index == \u001b[38;5;28mlen\u001b[39m(inp) - \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP_Project/Persian-Turkish-NLP/.venv/lib/python3.12/site-packages/zemberek/normalization/character_graph_decoder.py:216\u001b[39m, in \u001b[36mCharacterGraphDecoder.Hypothesis.get_new_move_forward\u001b[39m\u001b[34m(self, node, penalty_to_add, op)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_new_move_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, node: Node, penalty_to_add: \u001b[38;5;28mfloat\u001b[39m, op: \u001b[33m'\u001b[39m\u001b[33mCharacterGraphDecoder.Operation\u001b[39m\u001b[33m'\u001b[39m) -> \\\n\u001b[32m    215\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mCharacterGraphDecoder.Hypothesis\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCharacterGraphDecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHypothesis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mpenalty_to_add\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m                                            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mending\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchar_index\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP_Project/Persian-Turkish-NLP/.venv/lib/python3.12/site-packages/zemberek/normalization/character_graph_decoder.py:170\u001b[39m, in \u001b[36mCharacterGraphDecoder.Hypothesis.__init__\u001b[39m\u001b[34m(self, previous, node, penalty, operation, word, ending, char_index)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mHypothesis\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, previous, node: Node, penalty: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[32m    171\u001b[39m                  operation: \u001b[33m'\u001b[39m\u001b[33mCharacterGraphDecoder.Operation\u001b[39m\u001b[33m'\u001b[39m, word, ending, char_index: \u001b[38;5;28mint\u001b[39m = -\u001b[32m1\u001b[39m):\n\u001b[32m    172\u001b[39m         \u001b[38;5;28mself\u001b[39m.previous = previous  \u001b[38;5;66;03m# previous: Hypothesis, word: str, ending: str\u001b[39;00m\n\u001b[32m    173\u001b[39m         \u001b[38;5;28mself\u001b[39m.node = node\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not DATA_DIR.is_dir():\n",
    "    logging.error(f\"Data directory not found: {DATA_DIR}\")\n",
    "    logging.error(\"Please create a 'data' directory and place your decade .txt files (e.g., 1930s.txt) in it.\")\n",
    "    raise FileNotFoundError(\"Data directory not found\")\n",
    "\n",
    "\n",
    "decade_files = list(DATA_DIR.glob(\"*.txt\"))\n",
    "sort(decade_files)\n",
    "\n",
    "if not decade_files:\n",
    "    logging.warning(f\"No .txt files found in {DATA_DIR}. Nothing to process.\")\n",
    "    raise FileNotFoundError(\"No .txt files found in data directory\")\n",
    "\n",
    "logging.info(f\"Found {len(decade_files)} decade files to process: {[f.name for f in decade_files]}\")\n",
    "\n",
    "preprocess_all_decade_files(decade_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_file in sorted(decade_files): \n",
    "    train_decade_model(text_file)\n",
    "\n",
    "logging.info(\"All decades processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2Nn7KEQejYy"
   },
   "outputs": [],
   "source": [
    "test_model_path = MODEL_OUTPUT_DIR / \"fasttext_1930s.model\"\n",
    "if not test_model_path.exists():\n",
    "    logging.error(f\"Test model {test_model_path} not found. Run training first.\")\n",
    "    exit(1)\n",
    "logging.info(f\"\\n--- Loading and testing model: {test_model_path} ---\")\n",
    "loaded_model = FastText.load(str(test_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vector for a word\n",
    "try:\n",
    "    word_vector = loaded_model.wv[\"cumhuriyet\"]\n",
    "    logging.info(f\"Vector for 'cumhuriyet': {word_vector[:5]}...\") # Print first 5 dims\n",
    "except KeyError:\n",
    "    logging.info(\"'cumhuriyet' not in vocabulary of 1930s model (or below min_count).\")\n",
    "\n",
    "logging.info(f\"Vector for OOV word '{oov_word}': {oov_vector[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most similar word\n",
    "word = 'mühür'\n",
    "try:\n",
    "    similar_words = loaded_model.wv.most_similar(word, topn=100)\n",
    "    print([t[0] for t in similar_words])\n",
    "except KeyError:\n",
    "    logging.info(f\"{word} is not in vocabulary of 1930s model (or below min_count).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText can also get vectors for OOV words if their n-grams are known\n",
    "oov_word = \"yepyenişeyler\" # A made-up word\n",
    "oov_vector = loaded_model.wv[oov_word]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
